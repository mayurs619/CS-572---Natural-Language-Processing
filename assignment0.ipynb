{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurs619/CS-572---Natural-Language-Processing/blob/main/assignment0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtKllKL-EFgx"
      },
      "source": [
        "## CS572 Assignment 0\n",
        "\n",
        "*This tutorial has been adapted from the one created by David Gaddy, Daniel Fried, Nikita Kitaev, Mitchell Stern, Rodolfo Corona, John DeNero, and Dan Klein.*\n",
        "\n",
        "The purpose of this first assignment is to make sure that you are familiar with all the tools you need to complete the programming assignments for the course.  We will walk you through the process of building a model with PyTorch in Colab.  Most of it will be structured as a tutorial, but we will ask you to fill in code and submit at the end.\n",
        "\n",
        "Please note that this assignment is not representative of the assignments for this course - other assignments will be considerably more involved and take longer to complete.\n",
        "\n",
        "**Grading rubric**\n",
        "- 60% Pooling network (meets target)\n",
        "- 40% Improved network (improvement over target)\n",
        "\n",
        "**Deadline**\n",
        "Monday 01/30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Important Note:\n",
        "In the TA Office hours next week, the TAs will use this assignment as a PyTorch tutorial and cover solutions to most parts. Please do attend the session. More details forthcoming on Ed.\n",
        "\n",
        "### TA contacts for this assignment:\n",
        "Gaurav Parikh (gaurav.rajesh.parikh@duke.edu) \\\\\n",
        "Aayush Sheth (aas146@duke.edu)"
      ],
      "metadata": {
        "id": "Zb2PO_cushkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "\n",
        "Our assignments will be given to you as Jupyter notebooks, and we intend for you to run them using Google Colab.\n",
        "Colab is an online editor that also provides free access to a GPU.\n",
        "To get started, make a copy of the assignment by clicking `File->Save a copy in drive...`.  You will need to be logged into a Google account.\n",
        "\n",
        "To access a GPU, go to `Edit->Notebook settings` and in the `Hardware accelerator` dropdown choose `GPU`.\n",
        "As soon as you run a code cell, you will be connected to a cloud instance with a GPU.\n",
        "Try running the code cell below to check that a GPU is connected (select the cell then either click the play button at the top left or press `Ctrl+Enter` or `Shift+Enter`)."
      ],
      "metadata": {
        "id": "jYSnaoOQrcDX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82H_i6foD8A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8308de7e-17b5-4841-8646-c48933d5c1bc"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('Found GPU')\n",
        "else:\n",
        "    print('Did not find GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GJMaS91RICk"
      },
      "source": [
        "When you run a code cell, Colab executes it on a temporary cloud instance.  Every time you open the notebook, you will be assigned a different machine.  All compute state and files saved on the previous machine will be lost.  Therefore, you may need to re-download datasets or rerun code after a reset. If you save output files that you don't want to lose, you should download them to your personal computer before moving on to something else.  You can download files by clicking the folder icon at the top left of the page under the menus to expand the sidebar and right clicking the file you want, and clicking `Download`.  Alternatively, you can mount your Google drive to the temporary cloud instance's local filesystem using the following code snippet and save files under the specified directory (note that you will have to provide permission every time you run this).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1et0WnGSVUD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4402f1f0-4aef-4732-d5f3-9c5a50eb088a"
      },
      "source": [
        "# mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# now you can see files\n",
        "!echo -e \"\\nNumber of Google drive files in /content/drive/My Drive/:\"\n",
        "!ls -l \"/content/drive/My Drive/\" | wc -l\n",
        "# by the way, you can run any linux command by putting a ! at the start of the line\n",
        "\n",
        "# by default everything gets executed and saved in /content/\n",
        "!echo -e \"\\nCurrent directory:\"\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Number of Google drive files in /content/drive/My Drive/:\n",
            "117\n",
            "\n",
            "Current directory:\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFp3VopnW3lZ"
      },
      "source": [
        "Many of the assignments will require training a model for some period of time, often on the order of 20-30 minutes.  There are some important limitations to Colab that you should be aware of when running code for this amount of time.  If you close the window or put your computer to sleep, Colab will disconnect you from the compute machine and your code will stop running.   There are also timeouts for inactivity (somewhere on the order of 30 minutes), so if you want to leave code running, be sure to check back periodically.  After a timeout, your compute machine will be disconnected and the files on it will be lost.\n",
        "\n",
        "A few other notes about using Colab:\n",
        "* The `Runtime` menu has many different run options, such as `Run all` or `Run after` so you don't have to run each code block individually.\n",
        "* Some people have run into CUDA device assert errors that did not originate from their code.  Restarting the runtime should fix this (unless there actually is a problem with your code).\n",
        "\n",
        "**Note on GPU usage**: Colab places some restrictions on GPU usage due to which you might get locked out after continuously using one (~8 hours). To avoid this, you should only use the GPU when needed. You can enable / disable GPU usage by changing the Runtime type under the Runtime menu. If you do get locked out of using a GPU, a potential workaround is to sign in using a different account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlDcpSk3VW5l"
      },
      "source": [
        "### Part-of-Speech Tagging\n",
        "\n",
        "You'll be trying to predict the most common [part of speech](https://web.stanford.edu/~jurafsky/slp3/8.pdf) for a word from its characters.  This project will focus on word types rather than tokens and not use any context (https://en.wikipedia.org/wiki/Type%E2%80%93token_distinction). This task is different from (and simpler than) a standard part-of-speech tagging task, which predicts part-of-speech tags for tokens in their sentential context.\n",
        "\n",
        "Many words can have multiple different parts of speech, but in this project we will associate each word only with its most common part of speech in the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), which has been manually labeled with part-of-speech tags.  \n",
        "\n",
        "Words are lowercased and filtered for length and frequency. Punctuation and numbers are removed. Any real NLP application would have to deal with the actual contents of text instead of filtering in this way, but we're just warming up.\n",
        "\n",
        "Below, we provide you with code to load the dataset. Please don't change the cell below, or you may confuse our autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T1ijH6-WlSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f739f97a-b21e-4f6d-f4e7-4bdd5b822338"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict, Counter\n",
        "import pickle\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "brown_tokens = brown.tagged_words(tagset='universal')\n",
        "print('Tagged tokens example: ', brown_tokens[:5])\n",
        "print('Total # of word tokens:', len(brown_tokens))\n",
        "\n",
        "max_word_len = 20\n",
        "\n",
        "def most_common(s):\n",
        "    \"Return the most common element in a sequence.\"\n",
        "    return Counter(s).most_common(1)[0][0]\n",
        "\n",
        "def most_common_tags(tagged_words, min_count=3, max_len=max_word_len):\n",
        "    \"Return a dictionary of the most common tag for each word, filtering a bit.\"\n",
        "    counts = defaultdict(list)\n",
        "    for w, t in tagged_words:\n",
        "        counts[w.lower()].append(t)\n",
        "    return {w: most_common(tags) for w, tags in counts.items() if\n",
        "            w.isalpha() and len(w) <= max_len and len(tags) >= min_count}\n",
        "\n",
        "brown_types = most_common_tags(brown_tokens)\n",
        "print('Tagged types example: ', sorted(brown_types.items())[:5])\n",
        "print('Total # of word types:', len(brown_types))\n",
        "\n",
        "def split(items, test_size):\n",
        "    \"Randomly split into train, validation, and test sets with a fixed seed.\"\n",
        "    random.Random(288).shuffle(items)\n",
        "    once, twice = test_size, 2 * test_size\n",
        "    return items[:-twice], items[-twice:-once], items[-once:]\n",
        "\n",
        "val_test_size = 1000\n",
        "all_data_raw = split(sorted(brown_types.items()), val_test_size)\n",
        "train_data_raw, validation_data_raw, test_data_raw = all_data_raw\n",
        "all_tags = sorted(set(brown_types.values()))\n",
        "print('Tag options:', all_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged tokens example:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of word tokens: 1161192\n",
            "Tagged types example:  [('a', 'DET'), ('aaron', 'NOUN'), ('ab', 'NOUN'), ('abandon', 'VERB'), ('abandoned', 'VERB')]\n",
            "Total # of word types: 18954\n",
            "Tag options: ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIx-VMZT7zSn"
      },
      "source": [
        "You're welcome to insert additional cells and explore the data. Our autograders don't rely on any particular structure of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJkfwD37w1G"
      },
      "source": [
        "# Explore the data here as you see fit."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZ4q4aqVRVF"
      },
      "source": [
        "First, let's run a baseline that predicts `NOUN` for every word. A predictor function takes a list of tagged words and returns a list of predicted tags. We've also provided some helper functions here to evaluate model outputs.  You don't need to fill in any code in this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvoPZ609WBpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85607bf5-4f56-4249-ab57-2aa7b1f496cc"
      },
      "source": [
        "def noun_predictor(raw_data):\n",
        "    \"A predictor that always predicts NOUN.\"\n",
        "    predictions = []\n",
        "    for word, _ in raw_data:\n",
        "        predictions.append('NOUN')\n",
        "    return predictions\n",
        "\n",
        "def accuracy(predictions, targets):\n",
        "    \"\"\"Return the accuracy percentage of a list of predictions.\n",
        "\n",
        "    predictions has only the predicted tags\n",
        "    targets has tuples of (word, tag)\n",
        "    \"\"\"\n",
        "    assert len(predictions) == len(targets)\n",
        "    n_correct = 0\n",
        "    for predicted_tag, (word, gold_tag) in zip(predictions, targets):\n",
        "        if predicted_tag == gold_tag:\n",
        "            n_correct += 1\n",
        "\n",
        "    return n_correct / len(targets) * 100.0\n",
        "\n",
        "def evaluate(predictor, raw_data):\n",
        "    return accuracy(predictor(raw_data), raw_data)\n",
        "\n",
        "def print_sample_predictions(predictor, raw_data, k=10):\n",
        "    \"Print the first k predictions.\"\n",
        "    d = raw_data[:k]\n",
        "    print('Sample predictions:',\n",
        "          [(word, guess) for (word, _), guess in zip(d, predictor(d))])\n",
        "\n",
        "print('noun baseline validation accuracy:',\n",
        "      evaluate(noun_predictor, validation_data_raw))\n",
        "print_sample_predictions(noun_predictor, validation_data_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noun baseline validation accuracy: 55.1\n",
            "Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'NOUN'), ('downwind', 'NOUN'), ('exodus', 'NOUN'), ('avoiding', 'NOUN'), ('informal', 'NOUN'), ('padded', 'NOUN'), ('tantalizing', 'NOUN'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIxSaG7pavpS"
      },
      "source": [
        "### Building a PyTorch Classifier\n",
        "\n",
        "We will be using the deep learning framework PyTorch for all our assignments.\n",
        "If you haven't used PyTorch at all before, we recommend you check out the tutorials on the PyTorch website: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html.  Throughout this assignment and the others in this course, you will need to reference the documentation at https://pytorch.org/docs/stable/index.html.  We'll be using PyTorch version 2.1.0, which comes pre-installed with Colab.  In this assignment, we'll walk you through the process of defining and training your neural network model, but future assignments will have less guidance.\n",
        "\n",
        "Below, we've provided a baseline network as a PyTorch Module that will learn a single parameter per part-of-speech tag. This model has the capacity to learn that `'NOUN'` is the most common tag and predict that. It can't do better. Use this network as you are developing your training and prediction code, then replace it with your actual network later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N8wiRpoiZzG"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaselineNetwork(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        # learn a vector of size n_outputs, initialized with all zeros\n",
        "        self.param = nn.Parameter(torch.zeros(n_outputs))\n",
        "\n",
        "    def forward(self, chars, mask):\n",
        "        \"\"\"This function defines the computation graph.\n",
        "\n",
        "        Args:\n",
        "          chars: a matrix of size batch_size x max_len\n",
        "          mask: a matrix of size batch_size x max_len\n",
        "\n",
        "        Returns:\n",
        "          a matrix of scores of size batch_size x n_outputs\n",
        "        \"\"\"\n",
        "        # return the same outputs (self.param) for each example in a batch\n",
        "        # here we use 'expand' to return the same outputs for each example in the batch\n",
        "        return self.param.expand(chars.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnJJfdiR8l_L"
      },
      "source": [
        "To train or evaluate a neural model, we'll need to transform the raw data from strings into tensors.  We've provided the following function to perform the transformation for you. Each word is prepended with the `^` character and appended with `$` so that these boundary characters are available to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f62L4sTrUEn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d880f278-8635-4c79-d0c9-ccb91d6d92d6"
      },
      "source": [
        "def make_matrices(data_raw):\n",
        "    \"\"\"Convert a list of (word, tag) pairs into tensors with appropriate padding.\n",
        "\n",
        "    character_matrix holds character codes for each word,\n",
        "      indexed as [word_index, character_index]\n",
        "    character_mask masks valid characters (1 for valid, 0 invalid),\n",
        "      indexed similarly so that all inputs can have a constant length\n",
        "    pos_labels holds part-of-speech one-hot vectors,\n",
        "      indexed as [word_index, pos_index] with 0/1 values\n",
        "    \"\"\"\n",
        "    max_len = max_word_len + 2  # leave room for word start/end symbols\n",
        "    character_matrix = torch.zeros(len(data_raw), max_len, dtype=torch.int64)\n",
        "    character_mask = torch.zeros(len(data_raw), max_len, dtype=torch.float32)\n",
        "    pos_labels = torch.zeros(len(data_raw), dtype=torch.int64)\n",
        "    for word_i, (word, pos) in enumerate(data_raw):\n",
        "        for char_i, c in enumerate('^' + word + '$'):\n",
        "            character_matrix[word_i, char_i] = ord(c)\n",
        "            character_mask[word_i, char_i] = 1\n",
        "        pos_labels[word_i] = all_tags.index(pos)\n",
        "    return torch.utils.data.TensorDataset(character_matrix, character_mask, pos_labels)\n",
        "\n",
        "validation_data = make_matrices(validation_data_raw)\n",
        "\n",
        "print('Sample datapoint after preprocessing:', validation_data[0])\n",
        "print('Raw datapoint:', validation_data_raw[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample datapoint after preprocessing: (tensor([ 94, 115,  97, 108, 101, 109,  36,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]), tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.]), tensor(5))\n",
            "Raw datapoint: ('salem', 'NOUN')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-FPqtw826J"
      },
      "source": [
        "The output of a `BaselineNetwork` is a matrix of dimension (batch_size, num_pos_labels) containing logits, or unnormalized log probabilities. To get probabilities from this matrix, you would run `F.softmax(x, dim=1)`, which exponentiates the logits and then normalizes each row to sum to 1.  The cell below generates an output distribution for the first example of the validation set, which is uniform because the network param was initialized to zero.\n",
        "\n",
        "In PyTorch, it is common to return pre-activation values from modules (e.g. the values before running the final softmax or sigmoid operation).  PyTorch has loss functions that combine the softmax/sigmoid operation into the loss operation for more numerical stability.  Be sure you know what type of values a network returns, as this will affect your training and prediction code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtr9xZXVB1Qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e2f8f8-2d9d-4b52-f9c3-1c1226bc35d6"
      },
      "source": [
        "# Create a network and copy its parameters to the GPU.\n",
        "untrained_baseline = BaselineNetwork(len(all_tags)).cuda()\n",
        "untrained_baseline.eval()\n",
        "\n",
        "# Select the first validation example.\n",
        "example = validation_data[0]\n",
        "chars, mask, _ = example\n",
        "\n",
        "# Networks only process batches. Create a batch of size one.\n",
        "chars_batch, mask_batch = chars.unsqueeze(0), mask.unsqueeze(0)\n",
        "\n",
        "# Copy batch to the GPU.\n",
        "chars_batch, mask_batch = chars_batch.cuda(), mask_batch.cuda()\n",
        "\n",
        "# Run the untrained network.\n",
        "logits = untrained_baseline(chars_batch, mask_batch)\n",
        "\n",
        "# Convert to a distribution.\n",
        "output_distribution = F.softmax(logits, dim=1).squeeze().tolist()\n",
        "\n",
        "# Inspect the distribution, which should be uniform.\n",
        "list(zip(all_tags, output_distribution))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ADJ', 0.09090909361839294),\n",
              " ('ADP', 0.09090909361839294),\n",
              " ('ADV', 0.09090909361839294),\n",
              " ('CONJ', 0.09090909361839294),\n",
              " ('DET', 0.09090909361839294),\n",
              " ('NOUN', 0.09090909361839294),\n",
              " ('NUM', 0.09090909361839294),\n",
              " ('PRON', 0.09090909361839294),\n",
              " ('PRT', 0.09090909361839294),\n",
              " ('VERB', 0.09090909361839294),\n",
              " ('X', 0.09090909361839294)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ubdajTtCwzt"
      },
      "source": [
        "Finally, time to write some code!\n",
        "\n",
        "In the cell below, define a predictor for a network by following the instructions in the comments. The predictor takes a list of words (strings) and returns a list of part-of-speech tags (also strings).\n",
        "\n",
        "For this assignment, we've provided more fine-grained instructions as comments in the code template.  You are free to explore methods and architectures other than the ones we specified in the comments, but we highly recommend starting with them, as they will help you reach the required accuracies and give lots of best practices to use in later projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlWQk-dcp9BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d004a9d-d68b-4fcb-c4bb-198ca0d23111"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def predict_using(network):\n",
        "    def predictor(raw_data):\n",
        "        \"\"\"Return a list of part-of-speech tags as strings, one for each word.\"\"\"\n",
        "\n",
        "        device = next(network.parameters()).device  # Get the device of the model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            network.eval()\n",
        "\n",
        "            input_data = make_matrices(raw_data)\n",
        "            data_loader = torch.utils.data.DataLoader(input_data, batch_size=32, shuffle=False)\n",
        "\n",
        "            predictions = []\n",
        "\n",
        "            for batch in data_loader:\n",
        "                chars_batch, mask_batch, _ = batch\n",
        "\n",
        "                chars_batch = chars_batch.to(device)\n",
        "                mask_batch = mask_batch.to(device)\n",
        "\n",
        "                outputs = network(chars_batch, mask_batch)\n",
        "\n",
        "                predicted_labels = outputs.argmax(dim=-1)\n",
        "\n",
        "                predicted_tags = [all_tags[label] for label in predicted_labels.cpu().numpy()]\n",
        "                predictions.extend(predicted_tags)\n",
        "\n",
        "            return predictions\n",
        "\n",
        "    return predictor\n",
        "\n",
        "\n",
        "\n",
        "print_sample_predictions(predict_using(untrained_baseline), validation_data_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions: [('salem', 'ADJ'), ('unsympathetic', 'ADJ'), ('downwind', 'ADJ'), ('exodus', 'ADJ'), ('avoiding', 'ADJ'), ('informal', 'ADJ'), ('padded', 'ADJ'), ('tantalizing', 'ADJ'), ('farce', 'ADJ'), ('berger', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_9jzlbFB1hw"
      },
      "source": [
        "\n",
        "Fill in the training function for the neural network below. This function should train any network.  \n",
        "\n",
        "Then, you'll have all the parts needed to train and evaluate the baseline network.  You should get the same accuracy as the all-noun baseline.  Make sure your train function prints validation scores so that you see score outputs here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ce8kZd-5pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef47c40-5072-48e7-b230-a8d55003f552"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "\n",
        "def train(network, n_epochs=25):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    network = network.to(device)\n",
        "\n",
        "    train_data = make_matrices(train_data_raw)\n",
        "    data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
        "\n",
        "    predictor = predict_using(network)\n",
        "\n",
        "    best_validation_score = float('-inf')\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch', epoch)\n",
        "\n",
        "        network.train()\n",
        "\n",
        "        for batch in tqdm.tqdm(data_loader, leave=False):\n",
        "            chars_batch, mask_batch, pos_batch = batch\n",
        "\n",
        "            chars_batch = chars_batch.to(device)\n",
        "            mask_batch = mask_batch.to(device)\n",
        "            pos_batch = pos_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = network(chars_batch, mask_batch)\n",
        "\n",
        "            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), pos_batch.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        validation_score = evaluate(predictor, validation_data_raw)\n",
        "        print('Validation score:', validation_score)\n",
        "\n",
        "        if validation_score > best_validation_score:\n",
        "            best_validation_score = validation_score\n",
        "            torch.save(network.state_dict(), 'best_model.pth')\n",
        "\n",
        "    network.load_state_dict(torch.load('best_model.pth'))\n",
        "    return network\n",
        "\n",
        "trained_baseline_network = train(BaselineNetwork(len(all_tags)), 2)\n",
        "print_sample_predictions(predict_using(trained_baseline_network), validation_data_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 55.1\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 55.1\n",
            "Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'NOUN'), ('downwind', 'NOUN'), ('exodus', 'NOUN'), ('avoiding', 'NOUN'), ('informal', 'NOUN'), ('padded', 'NOUN'), ('tantalizing', 'NOUN'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-aff1887ad452>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfDfOpTj41mj"
      },
      "source": [
        "It's time to actually define a non-trivial neural network.  We'll start with a pretty simple model that takes embeddings of the characters of a word, pools them, and runs a feedforward network.  Fill in your code for `PoolingNetwork` below.  A correct implementation should get a validation score over 66%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbJgzzO8SOJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ee877b-b3bc-48a8-c7c0-df327729350b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PoolingNetwork(nn.Module):\n",
        "    def __init__(self, n_outputs, embedding_dim=100, vocab_size=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, n_outputs)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, chars, mask):\n",
        "        device = chars.device\n",
        "        chars = chars.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        embeddings = self.embedding(chars)\n",
        "        embeddings = embeddings * mask.unsqueeze(-1)\n",
        "\n",
        "        valid_chars = mask.sum(dim=1).unsqueeze(-1)\n",
        "        pooled_embeddings = embeddings.sum(dim=1) / valid_chars\n",
        "\n",
        "        x = F.relu(self.linear1(pooled_embeddings))\n",
        "        x = self.dropout(x)\n",
        "        output = self.linear2(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "trained_pooling_network = train(PoolingNetwork(len(all_tags)))\n",
        "pooling_predictor = predict_using(trained_pooling_network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 60.8\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 63.9\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.10000000000001\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 64.60000000000001\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.60000000000001\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.7\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.10000000000001\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 64.8\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.8\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.8\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.7\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.60000000000001\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.8\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.5\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.4\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.2\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.8\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.8\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.9\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 65.3\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.10000000000001\n",
            "Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.7\n",
            "Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.0\n",
            "Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.8\n",
            "Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 66.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-aff1887ad452>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9iRzg47SUbD"
      },
      "source": [
        "And look at some outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv00vHSlSSbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fc7a6b-0cbe-437e-e7dd-3774bae26569"
      },
      "source": [
        "print_sample_predictions(pooling_predictor, validation_data_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'NOUN'), ('downwind', 'NOUN'), ('exodus', 'VERB'), ('avoiding', 'VERB'), ('informal', 'NOUN'), ('padded', 'VERB'), ('tantalizing', 'VERB'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-NnQ1Q4q-6U"
      },
      "source": [
        "For this next part, we'll give you a little more freedom to experiment.  Think about what types of information could be useful for predicting parts of speech.  Think about what the pooling model is missing. Try to improve the accuracy found in the previous run. Implement an improved model that reaches a validation score above 75%.\n",
        "\n",
        "One way is to operate over character n-grams before pooling.\n",
        "There are several ways to implement this, but if you need help, you can use the following steps between the creation of embeddings and the mask/pool operations to process bigrams:\n",
        "1. create two slices of the embedding tensor, one with the first character cut off and one with the last cut off\n",
        "2. concatenate the two sliced tensors along the embedding dimension with `torch.cat`\n",
        "3. run a linear layer with activation on the concatenated embeddings\n",
        "4. cut off the first character of the mask tensor\n",
        "\n",
        "You can also try playing around with the hyperparameters of the network and the optimizer to see if that improves accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hT0Fj_de2j5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c5c243-9079-4832-e180-7c728dcad335"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedNetwork(nn.Module):\n",
        "    def __init__(self, n_outputs, embedding_dim=200, vocab_size=1000, max_chars=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim * 3, 512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.linear3 = nn.Linear(256, n_outputs)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        nn.init.xavier_uniform_(self.linear2.weight)\n",
        "        nn.init.xavier_uniform_(self.linear3.weight)\n",
        "\n",
        "    def forward(self, chars, mask):\n",
        "        device = chars.device\n",
        "        chars = chars.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        embeddings = self.embedding(chars)\n",
        "\n",
        "        embeddings_first = embeddings[:, :-2, :]\n",
        "        embeddings_second = embeddings[:, 1:-1, :]\n",
        "        embeddings_third = embeddings[:, 2:, :]\n",
        "\n",
        "        trigrams = torch.cat([embeddings_first, embeddings_second, embeddings_third], dim=-1)\n",
        "\n",
        "        trigram_mask = mask[:, 2:]\n",
        "        trigram_mask = trigram_mask.unsqueeze(-1)\n",
        "\n",
        "        pooled = (trigrams * trigram_mask).sum(dim=1) / trigram_mask.sum(dim=1)\n",
        "\n",
        "        x = self.batch_norm1(self.linear1(pooled))\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.batch_norm2(self.linear2(x))\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        output = self.linear3(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "trained_improved_network = train(ImprovedNetwork(len(all_tags)))\n",
        "improved_predictor = predict_using(trained_improved_network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 72.5\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 74.2\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 75.5\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 75.8\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.2\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.3\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.1\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.0\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.7\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.8\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.1\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.4\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.6\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.1\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.3\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.6\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.5\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.7\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.9\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.6\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.0\n",
            "Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.5\n",
            "Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.2\n",
            "Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 76.5\n",
            "Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 77.10000000000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-aff1887ad452>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaTxzvNgxtDR"
      },
      "source": [
        "We can also get a feel for what our model learned by providing some of our own inputs that aren't real words (yet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFMX_RBKT1oN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd3849a-6e60-440c-f72e-382e201a677b"
      },
      "source": [
        "print_sample_predictions(improved_predictor, validation_data_raw)\n",
        "\n",
        "print_sample_predictions(improved_predictor, [['kleining','X'], ['deneroful','X']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'ADJ'), ('downwind', 'ADJ'), ('exodus', 'ADJ'), ('avoiding', 'VERB'), ('informal', 'ADJ'), ('padded', 'VERB'), ('tantalizing', 'VERB'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n",
            "Sample predictions: [('kleining', 'VERB'), ('deneroful', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrsWMTREe9EY"
      },
      "source": [
        "Next, you need to run your model on the test set and save the outputs.  You'll turn in your predictions for us to grade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRxZb1LBfVmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7922ea1-4232-4c3d-c23e-621cf1e38cd2"
      },
      "source": [
        "def save_predictions(predictions, filename):\n",
        "    \"\"\"Save predictions to a file.\n",
        "\n",
        "    predictions is a list of strings.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for pred in predictions:\n",
        "            f.write(pred)\n",
        "            f.write('\\n')\n",
        "\n",
        "print('test score pooling:', evaluate(pooling_predictor, test_data_raw))\n",
        "print('test score improved:', evaluate(improved_predictor, test_data_raw))\n",
        "\n",
        "test_predictions = pooling_predictor(test_data_raw)\n",
        "save_predictions(test_predictions, 'predicted_test_outputs_pooling.txt')\n",
        "test_predictions = improved_predictor(test_data_raw)\n",
        "save_predictions(test_predictions, 'predicted_test_outputs_improved.txt')\n",
        "\n",
        "# Check that your test set looks like we expect it to\n",
        "import hashlib\n",
        "m = hashlib.md5()\n",
        "m.update(str(test_data_raw).encode('utf-8'))\n",
        "assert m.digest() == b'*N\\xf6\\xbe\\xed\\xde\\xe8q)\\xb9GG\\xa6\\x15UI'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test score pooling: 69.8\n",
            "test score improved: 78.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will run our model on a hidden test set.\n",
        "- We first download the hidden test_set.\n",
        "- Run our trained models on it.\n",
        "- Submit the output files to gradescope"
      ],
      "metadata": {
        "id": "7ZL_x-xGlQca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading hidden test set.\n",
        "!pip install gdown==v4.6.3\n",
        "!gdown 1iDjTU-NpXBB-OFEow5s4jiD-URF6Wbmw\n",
        "hidden_test_words = pickle.load(open(\"hidden_words.p\", \"rb\"))\n",
        "assert hidden_test_words[0][0]=='reversed' and hidden_test_words[-1][0]=='under'"
      ],
      "metadata": {
        "id": "4JHIHEyqlNVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489112b3-791a-4185-c8f8-23b18a1e6ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown==v4.6.3 in /usr/local/lib/python3.11/dist-packages (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==v4.6.3) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==v4.6.3) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gdown==v4.6.3) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==v4.6.3) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==v4.6.3) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==v4.6.3) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==v4.6.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==v4.6.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==v4.6.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==v4.6.3) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==v4.6.3) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iDjTU-NpXBB-OFEow5s4jiD-URF6Wbmw\n",
            "To: /content/hidden_words.p\n",
            "100% 47.3k/47.3k [00:00<00:00, 89.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hidden_test_words is the new hidden dataset provided. All the labels in the hidden dataset are incorrectly assigned label 'X', the 'other' tag from the universal tagset."
      ],
      "metadata": {
        "id": "AcoqOhrHoEIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(hidden_test_words[:3])"
      ],
      "metadata": {
        "id": "ARoVVnBIoyhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3107679f-f426-42a7-ff89-6e6262a770c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reversed', 'X'), ('baum', 'X'), ('coffers', 'X')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your job now is to find the correct lables on this test set. We do that by running the following code."
      ],
      "metadata": {
        "id": "x_RqyxDCo5TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = pooling_predictor(hidden_test_words)\n",
        "save_predictions(test_predictions, 'predicted_hidden_test_outputs_pooling.txt')\n",
        "test_predictions = improved_predictor(hidden_test_words)\n",
        "save_predictions(test_predictions, 'predicted_hidden_test_outputs_improved.txt')"
      ],
      "metadata": {
        "id": "UTWPcHOrocCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit the corrected labels found onto gradescope. You will be evaluated on these labels and a final score will be given."
      ],
      "metadata": {
        "id": "kkHcbUA2rbJt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7Ku-YUSc0D"
      },
      "source": [
        "### Gradescope\n",
        "\n",
        "We will use Gradescope for assignment submission.  Please let us know if you are unable to access Gradescope for any reason.\n",
        "\n",
        "You will submit this notebook and the required output files that we specify.  For this project, your submission will contain:\n",
        "* assignment0.ipynb (rename this notebook to match this)\n",
        "* predicted_test_outputs_pooling.txt\n",
        "* predicted_test_outputs_improved.txt\n",
        "* predicted_hidden_test_outputs_pooling.txt\n",
        "* predicted_hidden_test_outputs_improved.txt\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "To download this notebook, go to `File->Download .ipynb`.  Please rename the file to match the name in our file list.  You can download other outputs, like `predicted_test_output_improved.txt` by clicking the > arrow near the top left and finding it under `Files`.\n",
        "\n",
        "When submitting your ipython notebooks, make sure everything runs correctly if the cells are executed in order starting from a fresh session.  Note that just because a cell runs in your current session doesn't mean it doesn't rely on code that you have already changed or deleted.  If the code doesn't take too long to run, we recommend re-running everything with `Runtime->Restart and run all...`.\n",
        "\n",
        "When you upload your submission to the Gradescope assignment, you should get immediate feedback that confirms your submission was processed correctly.  Be sure to check this, as an incorrectly formatted submission could cause the autograder to fail.  For this project, you should be able to see your test set accuracies and a confirmation that all required files were found.  Most assignments will be graded primarily on your test set accuracies, but we may also use other factors to grade.\n",
        "\n",
        "Note that Gradesope will allow you to submit multiple times before the deadline, and we will use the latest submission for grading."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DhxTmwhIskl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}