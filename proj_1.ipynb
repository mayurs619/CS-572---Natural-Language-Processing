{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayurs619/CS-572---Natural-Language-Processing/blob/main/proj_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zg1qMBn7YQk"
      },
      "source": [
        "## Assignment 1: Language Modeling\n",
        "\n",
        "*This assignment is adapted from one created by David Gaddy, Daniel Fried, Nikita Kitaev, Mitchell Stern, Rodolfo Corona, John DeNero, and Dan Klein.*\n",
        "\n",
        "TA contact for this assignment: Saahas Parise (saahas.parise@duke.edu),\n",
        "Xinchang Xiong (xinchang.xiong@duke.edu)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In this assignment, you will implement several different types of language models for text.  We'll start with n-gram models, then move on to neural n-gram and GRU language models.\n",
        "\n",
        "**Warning**: Do not start this project the day before it is due!  Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n",
        "\n",
        "Our dataset for this project will be the WikiText2 language modeling dataset.  This dataset comes with some of the basic preprocessing done for us, such as tokenization and rare word filtering (using the `<unk>` token).\n",
        "Therefore, we can assume that all word types in the test set also appear at least once in the training set.\n",
        "We'll also use the Huggingface `datasets` and `tokenizers` libraries to help with some of the data preprocessing, such as converting tokens into id numbers.\n",
        "\n",
        "**Note on GPU usage**: You will need to use a GPU for the neural n-gram and GRU models but **not** for the n-gram models. Colab places some restrictions on GPU usage due to which you might get locked out after continuously using one (~8 hours). To avoid this, you should only use the GPU when needed, i.e., on training and inference for the last two parts of this assignment. You can enable / disable GPU usage by changing the Runtime type under the Runtime menu.\n",
        "If you do get locked out of using a GPU, a potential workaround is to sign in using a different account.\n",
        "\n",
        "When training a model on the GPU it is also a good idea to save your model periodically in case you get locked out. You can use `torch.save(network.state_dict(), path)` and `network.load_state_dict()` for this; see [here](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html). You can also save your `*.npy` files to google drive to avoid lossing them after Colab session cut-off (see the sample script below).\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/CS572-S24-A1\n",
        "!cp *.npy /content/drive/MyDrive/CS572-S24-A1\n",
        "!ls /content/drive/MyDrive/CS572-S24-A1\n",
        "```\n",
        "\n",
        "**Grading rubric**\n",
        "- 70% results\n",
        " - 15% bigram_predictions.npy (correctness)\n",
        " - 15% trigram_kn_predictions.npy (correctness)\n",
        " - 15% neural_trigram_predictions.npy (meets target)\n",
        " - 15% gru_predictions.npy (meets target)\n",
        " - 10% gru_predictions.npy (improvement over target)\n",
        "  \n",
        "- 30% writeup\n",
        " - 12.5% clarity\n",
        " - 12.5% correctness\n",
        " - 5% interestingness of ideas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQZGzVihrDUw",
        "outputId": "55ff6988-fa68-485e-f621-73c555d8400c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# # Install some required packages.\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oXAsamCqtH3",
        "outputId": "aacfa382-a7eb-4f29-84e8-3f700bd2debb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>', 'Homarus', 'gammarus', ',', 'known', 'as', 'the', 'European', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'Atlantic', 'Ocean']\n"
          ]
        }
      ],
      "source": [
        "# This block handles some basic setup and data loading.\n",
        "# You shouldn't need to edit this, but if you want to\n",
        "# import other standard python packages, that is fine.\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# We'll use HuggingFace's datasets and tokenizers libraries, which are a bit\n",
        "# heavy-duty for what we're doing, but it's worth getting to know them.\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "tokenizer = Tokenizer(WordLevel(unk_token='<unk>'))\n",
        "tokenizer.pre_tokenizer = WhitespaceSplit() # should be equivalent to split()\n",
        "\n",
        "# \"Training\" a tokenizer below just feeds it all the tokens so it can map from\n",
        "# word type to id.\n",
        "\n",
        "# should only be 33,278 distinct types in Wikitext-2\n",
        "trainer = WordLevelTrainer(vocab_size=33300, special_tokens=[\"<unk>\", \"<eos>\"])\n",
        "generator_bsz = 512\n",
        "all_splits_generator = (dataset[split][i:i+generator_bsz][\"text\"]\n",
        "                        for split in [\"train\", \"validation\", \"test\"]\n",
        "                          for i in range (0, len(dataset[split]), generator_bsz))\n",
        "tokenizer.train_from_iterator(all_splits_generator, trainer)\n",
        "\n",
        "# If desired, we could make a transformers tokenizer object now with:\n",
        "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "\n",
        "orig_vocab = tokenizer.get_vocab() # The tokenizer reserves a <pad> id, which we'll ignore.\n",
        "word_types = sorted(list(orig_vocab.keys()), key=lambda w: orig_vocab[w]) # no <pad>\n",
        "vocab = {w: i for i, w in enumerate(word_types)} # no <pad>\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Make a single stream of tokens, with an <eos> after each newline.\n",
        "\n",
        "train_text = []\n",
        "for example in dataset[\"train\"][\"text\"]:\n",
        "  train_text.extend(tokenizer.encode(example).tokens + [\"<eos>\"])\n",
        "\n",
        "validation_text = []\n",
        "for example in dataset[\"validation\"][\"text\"]:\n",
        "  validation_text.extend(tokenizer.encode(example).tokens + [\"<eos>\"])\n",
        "\n",
        "print(validation_text[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g10PLGiZn0XY"
      },
      "source": [
        "We've implemented a unigram model here as a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7ZHMVZzoPEH",
        "outputId": "a54fe1dd-cb2c-4b7f-c93b-f79c9342426c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 965.0860734119312\n"
          ]
        }
      ],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, train_text):\n",
        "        self.counts = Counter(train_text)\n",
        "        self.total_count = len(train_text)\n",
        "\n",
        "    def probability(self, word):\n",
        "        return self.counts[word] / self.total_count\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "        return [self.probability(word) for word in word_types]\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\"Return the perplexity of the model on a text as a float.\n",
        "        full_text -- a list of string tokens\n",
        "        \"\"\"\n",
        "        log_probabilities = []\n",
        "        for word in full_text:\n",
        "            # Note that the base of the log doesn't matter as long as the log and exp use the same base.\n",
        "            log_probabilities.append(math.log(self.probability(word), 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "unigram_demonstration_model = UnigramModel(train_text)\n",
        "print('unigram validation perplexity:', unigram_demonstration_model.perplexity(validation_text))\n",
        "\n",
        "def check_validity(model):\n",
        "    \"\"\"Performs several sanity checks on your model:\n",
        "    1) That next_word_probabilities returns a valid distribution\n",
        "    2) That perplexity matches a perplexity calculated from next_word_probabilities\n",
        "\n",
        "    Although it is possible to calculate perplexity from next_word_probabilities,\n",
        "    it is still good to have a separate more efficient method that only computes\n",
        "    the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = validation_text[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab[validation_text[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        log_probabilities.append(math.log(selected_prob))\n",
        "\n",
        "    perplexity = math.exp(-np.mean(log_probabilities))\n",
        "    your_perplexity = model.perplexity(validation_text[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n",
        "\n",
        "\n",
        "check_validity(unigram_demonstration_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4esz5XrEpNo"
      },
      "source": [
        "To generate from a language model, we can sample one word at a time conditioning on the words we have generated so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNj5nl4E7Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7441375e-70cb-43f6-e0d0-d2105f56596e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> and spoke <unk> , <eos> win equals produced <unk> and Cabral flew but things placed either Reception she only to\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "    prefix = list(prefix)\n",
        "    for _ in range(n):\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        word = random.choices(word_types, probs)[0]\n",
        "        prefix.append(word)\n",
        "    return ' '.join(prefix)\n",
        "\n",
        "print(generate_text(unigram_demonstration_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq-WtaM6F6kN"
      },
      "source": [
        "In fact there are many strategies to get better-sounding samples, such as only sampling from the top-k words or sharpening the distribution with a temperature.  You can read more about sampling from a language model in this paper: https://arxiv.org/pdf/1904.09751.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuopg4rYjf2O"
      },
      "source": [
        "You will need to submit some outputs from the models you implement for us to grade.  The following will download the required output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kL3zQDVw7g9",
        "outputId": "3b7c735c-203f-48d5-9103-c5f7471a668d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aHC9RfmeSa8dDwC9XGQN8uRmtTwJaIRw\n",
            "To: /content/nu_eval_output_vocab_short.txt\n",
            "100% 3.67k/3.67k [00:00<00:00, 12.5MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oedI437UeS9AhUGwsC-PgBdD1sHOjhaY\n",
            "To: /content/nu_eval_prefixes_short.txt\n",
            "100% 104k/104k [00:00<00:00, 4.03MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1aHC9RfmeSa8dDwC9XGQN8uRmtTwJaIRw\n",
        "!gdown --id 1oedI437UeS9AhUGwsC-PgBdD1sHOjhaY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB6MKPbm4z9s"
      },
      "outputs": [],
      "source": [
        "def save_truncated_distribution(model, filename, short=True):\n",
        "    \"\"\"Generate a file of truncated distributions.\n",
        "\n",
        "    Probability distributions over the full vocabulary are large,\n",
        "    so we will truncate the distribution to a smaller vocabulary.\n",
        "\n",
        "    Please do not edit this function\n",
        "    \"\"\"\n",
        "    vocab_name = 'nu_eval_output_vocab'\n",
        "    prefixes_name = 'nu_eval_prefixes'\n",
        "\n",
        "    if short:\n",
        "      vocab_name += '_short'\n",
        "      prefixes_name += '_short'\n",
        "\n",
        "    with open('{}.txt'.format(vocab_name), 'r') as eval_vocab_file:\n",
        "        eval_vocab = [w.strip() for w in eval_vocab_file]\n",
        "    eval_vocab_ids = [vocab[s] for s in eval_vocab]\n",
        "\n",
        "    all_selected_probabilities = []\n",
        "    with open('{}.txt'.format(prefixes_name), 'r') as eval_prefixes_file:\n",
        "        lines = eval_prefixes_file.readlines()\n",
        "        #for line in tqdm.notebook.tqdm(lines, leave=False):\n",
        "        for line in lines:\n",
        "            prefix = line.strip().split(' ')\n",
        "            probs = model.next_word_probabilities(prefix)\n",
        "            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n",
        "            all_selected_probabilities.append(selected_probs)\n",
        "\n",
        "    all_selected_probabilities = np.stack(all_selected_probabilities)\n",
        "    np.save(filename, all_selected_probabilities)\n",
        "    print('saved', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nzVrTWcH67Q",
        "outputId": "beb01247-9dbe-4afa-fba8-c38481d17b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved unigram_demonstration_predictions.npy\n"
          ]
        }
      ],
      "source": [
        "save_truncated_distribution(unigram_demonstration_model,\n",
        "                            'unigram_demonstration_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpoBn9m8jw0p"
      },
      "source": [
        "**Before you proceed**: At this point you should check whether you are able to upload the submission files to Gradescope. For this we will generate *dummy* prediction files by copying the unigram predictions above. Download the `unigrm_demonstration_predictions.npy` (you can do this by clicking the folder icon on left menu) and then copy this file and rename it to generate the required submision files:\n",
        "* bigram_predictions.npy\n",
        "* trigram_kn_predictions.npy\n",
        "* neural_trigram_predictions.npy\n",
        "* gru_predictions.npy\n",
        "\n",
        "Also save a copy of this notebook as `proj_1.ipynb` and create a `report.pdf` (this can be empty for now). Upload these files to Gradescope and confirm that the autograder runs and produces an output score of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfUwbbS9vy0"
      },
      "source": [
        "### N-gram Model\n",
        "\n",
        "Now it's time to implement an n-gram language model.\n",
        "\n",
        "Because not every n-gram will have been observed in training, use add-alpha smoothing to make sure no output word has probability 0.\n",
        "\n",
        "$$P(w_2|w_1)=\\frac{\\#(w_1,w_2)+\\alpha}{\\#(w_1)+V\\alpha}$$\n",
        "\n",
        "where $V$ is the vocab size and $\\#()$ is the count for the given bigram.  An alpha value around `3e-3`  should work.  Later, we'll replace this smoothing with model backoff.\n",
        "\n",
        "One edge case you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You can handle this however you like as long as you produce a valid probability distribution, but just using a uniform distribution over the vocabulary is reasonable for the purposes of this project.\n",
        "\n",
        "A properly implemented bi-gram model should get a perplexity **below 505** on the validation set.\n",
        "\n",
        "**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass.\n",
        "Also, we suggest pre-computing and caching the counts $\\#()$ when you initialize `NGramModel` for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGnGpnPIXpTW",
        "outputId": "cfb89ee7-1659-4f02-cea1-923d4da6c49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 965.0913672756687\n",
            "bigram validation perplexity: 504.4260671566396\n",
            "trigram validation perplexity: 2965.602880102997\n",
            "saved bigram_predictions.npy\n"
          ]
        }
      ],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, train_text, n=2, alpha=3e-3):\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "\n",
        "        self.ngram_counts = {}\n",
        "        self.history_counts = {}\n",
        "\n",
        "        for i in range(len(train_text) - n + 1):\n",
        "            ngram = tuple(train_text[i : i+n])\n",
        "            history = tuple(train_text[i : i+n-1])\n",
        "\n",
        "            self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
        "            self.history_counts[history] = self.history_counts.get(history, 0) + 1\n",
        "\n",
        "    def _to_string(self, tokens):\n",
        "        return \"_\".join(tokens)\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        numerator = self.ngram_counts.get(n_gram, 0) + self.smoothing\n",
        "\n",
        "        if self.n == 1:\n",
        "            denominator = len(train_text) + (self.smoothing * vocab_size)\n",
        "        else:\n",
        "            history = n_gram[:-1]\n",
        "            denominator = self.history_counts.get(history, 0) + (self.smoothing * vocab_size)\n",
        "\n",
        "        probability = numerator / denominator\n",
        "        return probability\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        if self.n == 1:\n",
        "            return [self.n_gram_probability((word,)) for word in word_types]\n",
        "\n",
        "        if len(text_prefix) > self.n-1:\n",
        "            text_prefix = text_prefix[-(self.n-1):]\n",
        "\n",
        "        if len(text_prefix) < self.n-1:\n",
        "            return [1 / vocab_size] * vocab_size\n",
        "\n",
        "        return [\n",
        "            self.n_gram_probability(tuple(text_prefix) + (word,))\n",
        "            for word in word_types\n",
        "        ]\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        log_probs = []\n",
        "\n",
        "        for i in range(self.n-1):\n",
        "            ngram = full_text[0 : i]\n",
        "            prob = 1 / vocab_size\n",
        "            log_probs.append(math.log(prob, 2))\n",
        "\n",
        "        for i in range(self.n-1, len(full_text)):\n",
        "            ngram = tuple(full_text[i-self.n+1 : i+1])\n",
        "            prob = self.n_gram_probability(ngram)\n",
        "            log_probs.append(math.log(prob, 2))\n",
        "\n",
        "        perplexity = 2 ** -np.mean(log_probs)\n",
        "        return perplexity\n",
        "\n",
        "unigram_model = NGramModel(train_text, n=1)\n",
        "check_validity(unigram_model)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(validation_text))\n",
        "\n",
        "bigram_model = NGramModel(train_text, n=2)\n",
        "check_validity(bigram_model)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n",
        "\n",
        "trigram_model = NGramModel(train_text, n=3)\n",
        "check_validity(trigram_model)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(bigram_model, 'bigram_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRRLnk73-r9"
      },
      "source": [
        "\n",
        "Please download `bigram_predictions.npy` once you finish this section so that you can submit it.\n",
        "\n",
        "In the block below, please report your bigram validation perplexity.  (We will use this to help us calibrate our scoring on the test set.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEcUK27xVTcK"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Bigram validation perplexity: ***504.4260671561098***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6zgYw9VTx1"
      },
      "source": [
        "We can also generate samples from the model to get an idea of how it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2V-qHxB4yhS",
        "outputId": "1d3b8b1d-9cf1-4246-b5f9-8b3a4d08c0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> The Free Isesi outnumbered , although he had opposed to be a Bigger Protestants Vienna pitcher Copacabana accusing Taylor nevertheless\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(bigram_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsR8_Ch7AXAZ"
      },
      "source": [
        "We now free up some RAM, **it is important to run the cell below, otherwise you will likely run out of RAM in the Colab runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKt1ncf_ypz"
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXNlsEKb3Mz"
      },
      "source": [
        "This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting.\n",
        "$$P\\left(w_i|h_i\\right)=\\frac{max\\left\\{\\#(h_i, w_i)-d,0\\right\\}}{\\#(h_i)} + \\lambda(h_i) P(w_i|w_{i-n+2},\\ldots, w_{i-1})$$\n",
        "\n",
        "$$\\lambda\\left(h_i\\right)=\\frac{d N_{1+}(h_i)}{{\\#(h_i)}}$$\n",
        "where $h_i=(w_{i-n+1}, \\ldots, w_{i-1})$ is the prefix before token $i$, $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation).  If $\\#(h_i)=0$, use the lower order model probability directly (the above equations would have a division by 0).\n",
        "\n",
        "We found a discount $d$ of 0.9 to work well based on validation performance.  A trigram model with this discount value should get a validation perplexity below 272."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "BV4e4_mEc7VY",
        "outputId": "9ff5febd-8ae2-4b16-9000-da4bf0b95fd6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DiscountBackoffModel' object has no attribute 'context_counts'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-7681e419f632>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mbigram_backoff_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscountBackoffModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtrigram_backoff_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscountBackoffModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_backoff_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram_backoff_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trigram backoff validation perplexity:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigram_backoff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-565e5d039254>\u001b[0m in \u001b[0;36mcheck_validity\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_word_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Negative value in next_word_probabilities\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Value larger than 1 in next_word_probabilities\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-e9b8743bbbf4>\u001b[0m in \u001b[0;36mnext_word_probabilities\u001b[0;34m(self, text_prefix)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# If we're at the right size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-e9b8743bbbf4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# If we're at the right size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         return [\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         ]\n",
            "\u001b[0;32m<ipython-input-66-7681e419f632>\u001b[0m in \u001b[0;36mn_gram_probability\u001b[0;34m(self, n_gram)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_order_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DiscountBackoffModel' object has no attribute 'context_counts'"
          ]
        }
      ],
      "source": [
        "class DiscountBackoffModel(NGramModel):\n",
        "    def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n",
        "        super().__init__(train_text, n=n)\n",
        "        self.lower_order_model = lower_order_model\n",
        "        self.discount = delta\n",
        "\n",
        "        self.ngram_counts = Counter(\n",
        "            [tuple(train_text[i : i+self.n])\n",
        "             for i in range(len(train_text) - self.n + 1)]\n",
        "        )\n",
        "\n",
        "        self.history_counts = Counter(\n",
        "            [tuple(train_text[i:i+(self.n-1)])\n",
        "             for i in range(len(train_text)-(self.n-1)+1)]\n",
        "        )\n",
        "\n",
        "        self.N1s = {}\n",
        "        for i in range(len(train_text) - self.n + 1):\n",
        "            ndx = i + self.n - 1\n",
        "            prefix = tuple(train_text[i : ndx])\n",
        "            next_word = train_text[ndx]\n",
        "            if prefix in self.N1s:\n",
        "                self.N1s[prefix].add(next_word)\n",
        "            else:\n",
        "                self.N1s[prefix] = {next_word}\n",
        "        self.N1s = {k: len(v) for k, v in self.N1s.items()}\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        numerator = max(self.ngram_counts[n_gram] - self.discount, 0)\n",
        "        denominator = self.history_counts[n_gram[:-1]]\n",
        "\n",
        "        if denominator == 0:\n",
        "            return self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "\n",
        "        first_term = numerator / denominator\n",
        "\n",
        "        prefix = n_gram[:-1]\n",
        "        lambda_h1 = (self.discount * self.N1s[prefix]) / denominator\n",
        "        second_term = lambda_h1 * self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "\n",
        "        return first_term + second_term\n",
        "\n",
        "bigram_backoff_model = DiscountBackoffModel(train_text, unigram_model, 2)\n",
        "trigram_backoff_model = DiscountBackoffModel(train_text, bigram_backoff_model, 3)\n",
        "check_validity(trigram_backoff_model)\n",
        "print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(validation_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVrWYSMsBRSV"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WJe_trXBTjN"
      },
      "outputs": [],
      "source": [
        "# Release models we don't need any more.\n",
        "del unigram_model\n",
        "del bigram_backoff_model\n",
        "del trigram_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Xwrx8eO8i-"
      },
      "source": [
        "Now, implement Kneser-Ney to replace the unigram base model.\n",
        "$$P(w)\\propto |\\{w':\\#(w',w) > 0\\}|$$\n",
        "A Kneser-Ney trigram model should get a validation perplexity below 257."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g35KkB5ZBUW8"
      },
      "outputs": [],
      "source": [
        "class KneserNeyBaseModel(NGramModel):\n",
        "    def __init__(self, train_text):\n",
        "        super().__init__(train_text, n=1)\n",
        "\n",
        "        self.ngram_counts = Counter(\n",
        "            [tuple(train_text[i : i+self.n])\n",
        "             for i in range(len(train_text) - self.n + 1)]\n",
        "        )\n",
        "\n",
        "        self.cache = {}\n",
        "        for i in range(0, len(train_text)-2):\n",
        "            prefix = tuple(train_text[i : i+1],)\n",
        "            next_word = tuple(train_text[i+1 : i+2],)\n",
        "\n",
        "            if i == 0:\n",
        "                self.cache[prefix] = set()\n",
        "            elif next_word in self.cache:\n",
        "                self.cache[next_word].add(prefix)\n",
        "            else:\n",
        "                self.cache[next_word] = {prefix}\n",
        "\n",
        "        self.cache = {k: len(v) for k, v in self.cache.items()}\n",
        "        self.w_primes = sum(self.cache.values())\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == 1\n",
        "        return self.cache[n_gram] / self.w_primes\n",
        "\n",
        "kn_base = KneserNeyBaseModel(train_text)\n",
        "check_validity(kn_base)\n",
        "bigram_kn_backoff_model = DiscountBackoffModel(train_text, kn_base, 2)\n",
        "trigram_kn_backoff_model = DiscountBackoffModel(train_text, bigram_kn_backoff_model, 3)\n",
        "print('trigram Kneser-Ney backoff validation perplexity:', trigram_kn_backoff_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(trigram_kn_backoff_model, 'trigram_kn_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXcHXAjL43kd"
      },
      "outputs": [],
      "source": [
        "print(generate_text(trigram_kn_backoff_model))\n",
        "print(generate_text(trigram_kn_backoff_model, prefix=['What','about']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPecL2jMXQ3y"
      },
      "source": [
        "Fill in your trigram backoff perplexities with and without Kneser Ney."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIBVAMe0WV_1"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Trigram backoff validation perplexity: ***271.1216731108553***\n",
        "\n",
        "Trigram backoff with Kneser Ney perplexity: ***256.61334330211224***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3TFBf1CBiwp"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGprSePzBpLW"
      },
      "outputs": [],
      "source": [
        "# Delete models we don't need.\n",
        "del kn_base\n",
        "del bigram_kn_backoff_model\n",
        "del trigram_kn_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Y0S6XbB1iZ"
      },
      "source": [
        "### Neural N-gram Model\n",
        "\n",
        "In this section, you will implement a neural version of an n-gram model.  The model will use a simple feedforward neural network that takes the previous `n-1` words and outputs a distribution over the next word.\n",
        "\n",
        "You will use PyTorch to implement the model.  We've provided a little bit of code to help with the data loading using PyTorch's data loaders (https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "A model with the following architecture and hyperparameters should reach a validation perplexity **below or about 220**.\n",
        "* embed the words with dimension 128, then flatten into a single embedding for $n-1$ words (with size $(n-1)*128$)\n",
        "* run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total).\n",
        "* use weight tying for the embedding and final linear layer (this made a very large difference in our experiments); you can do this by creating the output layer with [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), then using [`F.embedding`](https://pytorch.org/docs/stable/generated/torch.nn.functional.embedding.html) with the linear layer's `.weight` to embed the input\n",
        "* rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers. **Note: You will likely find a performance drop if you add a nonlinear activation function after the dimension reduction layer.**\n",
        "* train for 10 epochs with the Adam optimizer (should take around 15-20 minutes)\n",
        "* do early stopping based on validation set perplexity (see Project 0)\n",
        "\n",
        "\n",
        "We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above.  A proper implementation with these should be enough to receive full credit on the assignment, although you might need to retrain the model for a few time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jokaz820Fk1h"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def ids(tokens):\n",
        "    return [vocab[t] for t in tokens]\n",
        "\n",
        "assert torch.cuda.is_available(), \"no GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "class NeuralNgramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        assert len(prev_token_ids) == self.n-1\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        vocab_size = len(vocab)\n",
        "\n",
        "        self.output_layer = nn.Linear(128, vocab_size)\n",
        "\n",
        "        self.fc1 = nn.Linear((n-1)*128, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.projection = nn.Linear(1024, 128)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = F.embedding(x, self.output_layer.weight)\n",
        "        embeds_flat = embeds.view(embeds.size(0), -1)\n",
        "\n",
        "        out = self.fc1(embeds_flat)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.projection(out)\n",
        "\n",
        "        logits = self.output_layer(out)\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.network = NeuralNGramNetwork(n).to(device)\n",
        "\n",
        "    def train(self, n_epochs=10):\n",
        "        dataset = NeuralNgramDataset(ids(train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        optimizer = optim.Adam(self.network.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            self.network.train()\n",
        "            total_loss = 0.0\n",
        "            total_tokens = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "            for x, y in progress_bar:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                log_probs = self.network(x)\n",
        "                loss = F.nll_loss(log_probs, y, reduction='sum')\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += y.size(0)\n",
        "                progress_bar.set_postfix({'loss': loss.item() / y.size(0)})\n",
        "\n",
        "            avg_loss = total_loss / total_tokens\n",
        "            train_ppl = math.exp(avg_loss)\n",
        "            val_ppl = self.perplexity(validation_text)\n",
        "            print(f\"Epoch {epoch+1}: Train PPL: {train_ppl:.2f}, Val PPL: {val_ppl:.2f}\")\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        self.network.eval()\n",
        "        prefix_ids = ids(text_prefix)\n",
        "        n = self.n\n",
        "        if len(prefix_ids) < n - 1:\n",
        "            padded = [vocab['<eos>']] * (n - 1 - len(prefix_ids)) + prefix_ids\n",
        "        else:\n",
        "            padded = prefix_ids[-(n-1):]\n",
        "        x = torch.tensor(padded, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            log_probs = self.network(x)\n",
        "        probs = torch.exp(log_probs).squeeze(0).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        self.network.eval()\n",
        "        dataset = NeuralNgramDataset(ids(text), self.n)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                log_probs = self.network(x)\n",
        "                loss = F.nll_loss(log_probs, y, reduction='sum')\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += y.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        return math.exp(avg_loss)\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3)\n",
        "check_validity(neural_trigram_model)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm-xW4FGXYYi"
      },
      "source": [
        "Fill in your neural trigram perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0cX0k2IW88k"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Neural trigram validation perplexity: ***211.05881961163914***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5PCZnkB1r5"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yH0lGOB1-S"
      },
      "outputs": [],
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOp1Gb_0WjlE"
      },
      "source": [
        "### RNN (GRU) Model\n",
        "\n",
        "For this stage of the project, you will implement an RNN language model using a popular variant -- [Gated recurrent unit (GRU)](https://en.wikipedia.org/wiki/Gated_recurrent_unit).\n",
        "\n",
        "For recurrent language modeling, the data batching strategy is a bit different from what is used in some other tasks.  Sentences are concatenated together so that one sentence starts right after the other, and an unfinished sentence will be continued in the next batch.  We have provided a helper class `RecurrentLMDataset` to this for you.  To properly deal with this input format, you should save the last state of the GRU from a batch to feed in as the first state of the next batch.  When you save state across different batches, you should call `.detach()` on the state tensors before the next batch to tell PyTorch not to backpropagate gradients through the state into the batch you have already finished (which will cause a runtime error).\n",
        "\n",
        "We expect your model to reach a validation perplexity **below 130**.  The following architecture and hyperparameters should be sufficient to get there.\n",
        "* 2 GRU layers with 768 units\n",
        "* dropout of 0.5 after each GRU layer and each linear layer (use different dropout modules)\n",
        "* instead of projecting directly from the last GRU output to the vocabulary size for softmax, project down to a smaller size first (e.g. 768->128->vocab_size). **NOTE: You may find that adding nonlinearities between these layers can hurt performance, try without first.**\n",
        "* use the same weights for the embedding layer and the pre-softmax layer; dimension 128\n",
        "* train with Adam (using default learning rates) for at least 10 epochs\n",
        "* clip gradient norms to be lower than 5 before taking an optimization step (code example below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOLXKKoc7If"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class RecurrentLMDataset:\n",
        "    def __init__(self, text_token_ids, bsz, bptt_len=32):\n",
        "        self.bsz = bsz\n",
        "        self.bptt_len = bptt_len\n",
        "        token_ids = torch.tensor(text_token_ids)\n",
        "        ncontig = token_ids.size(0) // bsz\n",
        "        token_ids = token_ids[:ncontig * bsz].view(bsz, -1)  # bsz x ncontig\n",
        "        self.token_ids = token_ids.t().contiguous()  # ncontig x bsz\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.ceil(self.token_ids.size(0) / self.bptt_len))\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, self.token_ids.size(0) - 1, self.bptt_len):\n",
        "            seqlen = min(self.bptt_len, self.token_ids.size(0) - i - 1)\n",
        "            x = self.token_ids[i:i + seqlen]      # seqlen x bsz\n",
        "            y = self.token_ids[i + 1:i + seqlen + 1]  # seqlen x bsz\n",
        "            yield x, y\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "bptt_len = 32\n",
        "n_layers = 2\n",
        "hidden_size = 768\n",
        "embed_size = 128\n",
        "clip = 5.0\n",
        "\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.dropout_value = 0.5\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.gru1 = nn.GRU(input_size=self.embed_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=1,\n",
        "                           bias=True)\n",
        "        self.gru1_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.gru2 = nn.GRU(input_size=self.hidden_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=1,\n",
        "                           bias=True)\n",
        "        self.gru2_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, self.embed_size)\n",
        "        self.fc_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.embedding_weight = nn.Parameter(torch.Tensor(self.vocab_size, self.embed_size))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding_weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.fc.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        embedded = F.embedding(x, self.embedding_weight)\n",
        "        embedded = self.emb_dropout(embedded)\n",
        "\n",
        "        out1, h1 = self.gru1(embedded, state[0:1])\n",
        "        out1 = self.gru1_dropout(out1)\n",
        "\n",
        "        out2, h2 = self.gru2(out1, state[1:2])\n",
        "        out2 = self.gru2_dropout(out2)\n",
        "\n",
        "        fc_out = self.fc(out2)\n",
        "        fc_out = self.fc_dropout(fc_out)\n",
        "        logits = F.linear(fc_out, self.embedding_weight)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        new_state = torch.cat([h1, h2], dim=0)\n",
        "        new_state = new_state.detach()\n",
        "        return log_probs, new_state\n",
        "\n",
        "class GRUModel:\n",
        "    def __init__(self):\n",
        "        self.network = GRUNetwork().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        optimizer = optim.Adam(self.network.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                         factor=0.5, patience=1, verbose=True)\n",
        "        train_dataset = RecurrentLMDataset(ids(train_text), batch_size, bptt_len)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            self.network.train()\n",
        "            total_loss = 0.0\n",
        "            total_batches = 0\n",
        "            state = torch.zeros(n_layers, batch_size, hidden_size, device=device)\n",
        "            epoch_iterator = tqdm(train_dataset, total=len(train_dataset),\n",
        "                                  desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "            for x, y in epoch_iterator:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                log_probs, state = self.network(x, state)\n",
        "                loss = F.nll_loss(log_probs.view(-1, self.network.vocab_size), y.view(-1))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "                epoch_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "            avg_train_loss = total_loss / total_batches\n",
        "            val_ppl = self.perplexity(validation_text)\n",
        "            print(f\"Epoch {epoch+1}: Training Loss: {avg_train_loss:.4f} | Validation Perplexity: {val_ppl:.2f}\")\n",
        "            scheduler.step(val_ppl)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "        self.network.eval()\n",
        "        token_list = ids(text_prefix)\n",
        "        if len(token_list) == 0:\n",
        "            raise ValueError(\"Text prefix is empty. Please provide a non-empty prefix.\")\n",
        "        token_ids = torch.tensor(token_list, dtype=torch.long, device=device).unsqueeze(1)\n",
        "        state = torch.zeros(n_layers, 1, hidden_size, device=device)\n",
        "        with torch.no_grad():\n",
        "            for i in range(token_ids.size(0)):\n",
        "                out, state = self.network(token_ids[i:i+1], state)\n",
        "            next_word_log_probs = out[-1, 0, :]\n",
        "            next_word_probs = torch.exp(next_word_log_probs)\n",
        "        return next_word_probs.cpu().numpy().tolist()\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        \"Return perplexity (a float) computed over the provided text.\"\n",
        "        self.network.eval()\n",
        "        val_dataset = RecurrentLMDataset(ids(text), batch_size, bptt_len)\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        state = torch.zeros(n_layers, batch_size, hidden_size, device=device)\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(val_dataset, total=len(val_dataset), desc=\"Evaluating\"):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                out, state = self.network(x, state)\n",
        "                loss = F.nll_loss(out.view(-1, self.network.vocab_size),\n",
        "                                  y.view(-1),\n",
        "                                  reduction='sum')\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += y.numel()\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        return math.exp(avg_loss)\n",
        "\n",
        "gru_model = GRUModel()\n",
        "gru_model.train()\n",
        "\n",
        "print('gru validation perplexity:', gru_model.perplexity(validation_text))\n",
        "save_truncated_distribution(gru_model, 'gru_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pGhdPQqHx9v"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Fill in your GRU perplexity.\n",
        "\n",
        "GRU validation perplexity: ***126.03939457275233***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoiXBWMaSPc"
      },
      "source": [
        "# Experimentation: 1-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to reach a validation perplexity **below 120**. You may either modify the GRU class above, or copy it down to the code cell below and modify it there. Just **be sure to run code cell below to generate results with your improved GRU**.\n",
        "\n",
        "**NOTE:** We will award at least 7 of the 10 points if your improved model reaches a perplexity below 140 on the hidden test cases on Gradescope.\n",
        "\n",
        "It is okay if the bulk of your improvements are due to hyperparameter tuning (such as changing the number or size of layers, adding dropout to more outputs), but implement at least one more substantial change to the model.  Here are some ideas (several of which come from https://arxiv.org/pdf/1708.02182.pdf), they do not always work, so might need to experiment with multiple ideas before seeing an improvement:\n",
        "* weight-drop regularization - apply dropout to the weight matrices instead of activations\n",
        "* learning rate scheduling - decrease the learning rate during training\n",
        "* weight regularization - add a l2 regularization penalty on the weights of the networks\n",
        "* ensembling - average the predictions of several models trained with different initialization random seeds\n",
        "* embedding dropout - zero out the entire embedding for a random set of words in the embedding matrix\n",
        "* activation regularization - add a l2 regularization penalty on the activation of the GRU output\n",
        "* temporal activation regularization - add l2 regularization on the difference between the GRU output activations at adjacent timesteps\n",
        "\n",
        "You may notice that most of these suggestions are regularization techniques. This dataset is considered fairly small, so regularization is one of the best ways to improve performance.\n",
        "\n",
        "For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **1-page maximum** in length and should be submitted in PDF format.  You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n",
        "For full credit, your write-up should include:\n",
        "1.   A concise and precise description of the extension(s) that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing validation perplexities of your improvement to the original GRU.\n",
        "The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with!\n",
        "When you submit the file, please name it `report.pdf`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4fxj-BQTDgU"
      },
      "source": [
        "Run the cell below in order to train your improved GRU and evaluate it.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_244hhNP9PO"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class RecurrentLMDataset:\n",
        "    def __init__(self, text_token_ids, bsz, bptt_len=32):\n",
        "        self.bsz = bsz\n",
        "        self.bptt_len = bptt_len\n",
        "        token_ids = torch.tensor(text_token_ids)\n",
        "        ncontig = token_ids.size(0) // bsz\n",
        "        token_ids = token_ids[:ncontig * bsz].view(bsz, -1)  # bsz x ncontig\n",
        "        self.token_ids = token_ids.t().contiguous()  # ncontig x bsz\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.ceil(self.token_ids.size(0) / self.bptt_len))\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, self.token_ids.size(0) - 1, self.bptt_len):\n",
        "            seqlen = min(self.bptt_len, self.token_ids.size(0) - i - 1)\n",
        "            x = self.token_ids[i:i + seqlen]      # seqlen x bsz\n",
        "            y = self.token_ids[i + 1:i + seqlen + 1]  # seqlen x bsz\n",
        "            yield x, y\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "n_epochs = 15\n",
        "bptt_len = 32\n",
        "n_layers = 2\n",
        "hidden_size = 768\n",
        "embed_size = 128\n",
        "clip = 5.0\n",
        "\n",
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.dropout_value = 0.4\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.gru1 = nn.GRU(input_size=self.embed_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=1,\n",
        "                           bias=True)\n",
        "        self.gru1_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.gru2 = nn.GRU(input_size=self.hidden_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=1,\n",
        "                           bias=True)\n",
        "        self.gru2_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, self.embed_size)\n",
        "        self.fc_dropout = nn.Dropout(self.dropout_value)\n",
        "\n",
        "        self.embedding_weight = nn.Parameter(torch.Tensor(self.vocab_size, self.embed_size))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.embedding_weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.fc.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        embedded = F.embedding(x, self.embedding_weight)\n",
        "        embedded = self.emb_dropout(embedded)\n",
        "\n",
        "        out1, h1 = self.gru1(embedded, state[0:1])\n",
        "        out1 = self.gru1_dropout(out1)\n",
        "\n",
        "        out2, h2 = self.gru2(out1, state[1:2])\n",
        "        out2 = self.gru2_dropout(out2)\n",
        "\n",
        "        fc_out = self.fc(out2)\n",
        "        fc_out = self.fc_dropout(fc_out)\n",
        "        logits = F.linear(fc_out, self.embedding_weight)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        new_state = torch.cat([h1, h2], dim=0)\n",
        "        new_state = new_state.detach()\n",
        "        return log_probs, new_state\n",
        "\n",
        "class GRUModel:\n",
        "    def __init__(self):\n",
        "        self.network = GRUNetwork().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        optimizer = optim.Adam(self.network.parameters())\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                         factor=0.5, patience=1, verbose=True)\n",
        "        train_dataset = RecurrentLMDataset(ids(train_text), batch_size, bptt_len)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            self.network.train()\n",
        "            total_loss = 0.0\n",
        "            total_batches = 0\n",
        "            state = torch.zeros(n_layers, batch_size, hidden_size, device=device)\n",
        "            epoch_iterator = tqdm(train_dataset, total=len(train_dataset),\n",
        "                                  desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "            for x, y in epoch_iterator:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                log_probs, state = self.network(x, state)\n",
        "                loss = F.nll_loss(log_probs.view(-1, self.network.vocab_size), y.view(-1))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "                epoch_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "            avg_train_loss = total_loss / total_batches\n",
        "            val_ppl = self.perplexity(validation_text)\n",
        "            print(f\"Epoch {epoch+1}: Training Loss: {avg_train_loss:.4f} | Validation Perplexity: {val_ppl:.2f}\")\n",
        "            scheduler.step(val_ppl)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "        self.network.eval()\n",
        "        token_list = ids(text_prefix)\n",
        "        if len(token_list) == 0:\n",
        "            raise ValueError(\"Text prefix is empty. Please provide a non-empty prefix.\")\n",
        "        token_ids = torch.tensor(token_list, dtype=torch.long, device=device).unsqueeze(1)\n",
        "        state = torch.zeros(n_layers, 1, hidden_size, device=device)\n",
        "        with torch.no_grad():\n",
        "            for i in range(token_ids.size(0)):\n",
        "                out, state = self.network(token_ids[i:i+1], state)\n",
        "            next_word_log_probs = out[-1, 0, :]\n",
        "            next_word_probs = torch.exp(next_word_log_probs)\n",
        "        return next_word_probs.cpu().numpy().tolist()\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        \"Return perplexity (a float) computed over the provided text.\"\n",
        "        self.network.eval()\n",
        "        val_dataset = RecurrentLMDataset(ids(text), batch_size, bptt_len)\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        state = torch.zeros(n_layers, batch_size, hidden_size, device=device)\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(val_dataset, total=len(val_dataset), desc=\"Evaluating\"):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                out, state = self.network(x, state)\n",
        "                loss = F.nll_loss(out.view(-1, self.network.vocab_size),\n",
        "                                  y.view(-1),\n",
        "                                  reduction='sum')\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += y.numel()\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        return math.exp(avg_loss)\n",
        "\n",
        "gru_model = GRUModel()\n",
        "gru_model.train()\n",
        "\n",
        "print('gru validation perplexity:', gru_model.perplexity(validation_text))\n",
        "save_truncated_distribution(gru_model, 'gru_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTOfrCG8CRF"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* proj_1.ipynb (rename to match this exactly)\n",
        "* gru_predictions.npy (this should also include all improvements from your exploration)\n",
        "* neural_trigram_predictions.npy\n",
        "* trigram_kn_predictions.npy\n",
        "* bigram_predictions.npy\n",
        "* report.pdf\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set perplexities shown by the autograder are on a different scale from your validation set perplexities due to selecting different text and truncating the distribution.  Don't worry if the values seem worse. We will compare your perplexity on the test set to our model's perplexity and assign a score based on that."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}